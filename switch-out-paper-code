from __future__ import unicode_literals, print_function, division
from io import open
from sklearn import preprocessing

import torch
from torch.autograd import Variable
from torch import nn, Tensor

def hamming_distance_sample(sents, tau, bos_id, eos_id, pad_id, vocab_size):
    """
    Sample a batch of corrupted examples from sents.
    Args:
        sents:          Tensor [batch_size, n_steps]. The input sentences.
        tau:            Temperature.
        vocab_size:     to create valid samples.
    Returns:
        sampled_sents:  Tensor [batch_size, n_steps]. The corrupted sentences.
    """
    mask = torch.eq(sents, bos_id) | torch.eq(sents, eos_id) | torch.eq(sents, pad_id)
    lengths = mask.float().sum(dim=1)
    batch_size, n_steps = sents.size()
    # first, sample the number of words to corrupt for each sentence
    logits = torch.arange(n_steps)
    logits = logits.float().mul_(-1).unsqueeze(0).expand_as(sents).contiguous().masked_fill_(mask, -float("inf"))
    logits = Variable(logits)
    probs = torch.nn.functional.softmax(logits.mul_(tau), dim=1)
    num_words = torch.distributions.Categorical(probs).sample()
    # sample the corrupted positions.
    corrupt_pos = num_words.data.float().div_(lengths).unsqueeze(1).expand_as(sents).contiguous().masked_fill_(mask, 0)
    corrupt_pos = torch.bernoulli(corrupt_pos, out=corrupt_pos).byte()
    total_words = int(corrupt_pos.sum())
    # sample the corrupted values, which will be added to sents
    corrupt_val = torch.LongTensor(total_words)
    corrupt_val = corrupt_val.random_(1, vocab_size)
    corrupts = torch.zeros(batch_size, n_steps).long()
    corrupts = corrupts.masked_scatter_(corrupt_pos, corrupt_val)
    sampled_sents = sents.add(Variable(corrupts)).remainder_(vocab_size)
    print(sampled_sents)
    return sampled_sents

le = preprocessing.LabelEncoder()
# lut = nn.Embedding(24000, 64, padding_idx= -1)
# lines1 = open('data/en-es/Processed/test.processed.clean.en', encoding='utf-8').read().strip().split('\n')
lines1 = [['▁Parliament', '▁Does', '▁Not', '▁Support', '▁Amendment', '▁Free', 'ing', '▁T', 'y', 'mos', 'hen', 'ko', '']]
newline = []
for line in lines1:
    newline = le.fit_transform(line)
    newline = torch.LongTensor(newline).unsqueeze(0)
    # newline = lut(newline)
    print(newline)
    break
hamming_distance_sample(newline,0.95,1,2,-1,24000)
